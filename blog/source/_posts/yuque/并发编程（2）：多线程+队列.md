---
title: 并发编程（2）：多线程+队列
urlname: ryephc
date: 2020-07-22 11:43:36 +0800
tags: []
categories: []
---

_以爬虫程序为例，先爬取代理网站的代理 ip，存入数据库动作_
\_

```python
# 多线程，队列控制共享变量，队列是线程安全的
import re
import Queue
import threading

import requests
from mongoengine import NotUniqueError

from models import Proxy
from config import PROXY_REGEX, PROXY_SITES
from utils import fetch


def save_proxies(url):
    proxies = []
    try:
        r = fetch(url)
    except requests.exceptions.RequestException:
        return False
    addresses = re.findall(PROXY_REGEX, r.text)
    for address in addresses:
        proxy = Proxy(address=address)
        try:
            proxy.save()
        except NotUniqueError:
            pass
        else:
            proxies.append(address)
    return proxies


def cleanup():
    # 初始化数据库
    Proxy.drop_collection()


def save_proxies_with_queue2(in_queue, out_queue):
    # 通过队列获取url，解析数据入库，结果到输出队列
    while True:
        url = in_queue.get()
        rs = save_proxies(url)
        out_queue.put(rs)
        in_queue.task_done()  # 队列完成发送信号


def append_result(out_queue, result):
    # 输出队列的数据生成 变量
    while True:
        rs = out_queue.get()
        if rs:
            result.extend(rs)
        out_queue.task_done()


def use_thread_with_queue2():
    """
    输入队列：代理网站
    输出队列：解析后的代理ip输出
    """
    cleanup()
    in_queue = Queue.Queue()
    out_queue = Queue.Queue()

    for i in range(5):
        t = threading.Thread(target=save_proxies_with_queue2,
                             args=(in_queue, out_queue))
        # 设置守护线程，主线程结束，守护线程结束
        t.setDaemon(True)
        t.start()

    for url in PROXY_SITES:
        in_queue.put(url)

    result = []

    for i in range(5):
        t = threading.Thread(target=append_result,
                             args=(out_queue, result))
        t.setDaemon(True)
        t.start()

    in_queue.join()
    out_queue.join()
	# 结果存到变量里
    print len(result)


def save_proxies_with_queue(queue):
    while 1:
        url = queue.get()
        save_proxies(url)
        queue.task_done()  # 队列完成发送信号


def use_thread_with_queue():
    cleanup()
    queue = Queue.Queue()

    for i in range(5):
        t = threading.Thread(target=save_proxies_with_queue, args=(queue,))
        t.setDaemon(True)
        t.start()

    for url in PROXY_SITES:
        queue.put(url)

    queue.join()


if __name__ == '__main__':
    use_thread_with_queue2()
```

还可以通过线程池，减少创建、销毁线程的开销

```python
from multiprocessing.dummy import Pool as ThreadPool


def use_thread_pool():
    cleanup()
    pool = ThreadPool(5)
    pool.map(save_proxies, PROXY_SITES)
    pool.close()
    pool.join()
```
